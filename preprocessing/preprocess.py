# âœ… [0] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° spaCy ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
#!pip install spacy
#!python -m spacy download en_core_web_sm
#!pip install contractions

# âœ… [1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° ë° spaCy ì´ˆê¸°í™”
import pandas as pd
import re
import spacy

# spaCy ì˜ì–´ ëª¨ë¸ ë¡œë”©
nlp = spacy.load("en_core_web_sm")

# âœ… [2] ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("dataset/Hotel_Reviews.csv")
print("ë°ì´í„° ê°œìˆ˜:", len(df))
df.head()

# âœ… [3] ê°ì •ë³„ ë¦¬ë·° ë¶„ë¦¬ ë° í†µí•©
# ê¸ì • ë¦¬ë·°
pos_df = df[df['Positive_Review'].str.strip().str.lower() != 'no positive'][['Positive_Review']]
pos_df = pos_df.rename(columns={'Positive_Review': 'Review'})
pos_df['Sentiment'] = 1

# ë¶€ì • ë¦¬ë·°
neg_df = df[df['Negative_Review'].str.strip().str.lower() != 'no negative'][['Negative_Review']]
neg_df = neg_df.rename(columns={'Negative_Review': 'Review'})
neg_df['Sentiment'] = 0

# ë³‘í•© ë° ì •ë¦¬
df_clean = pd.concat([pos_df, neg_df], ignore_index=True)
df_clean.dropna(subset=['Review'], inplace=True)
df_clean = df_clean.reset_index(drop=True)

print("ìµœì¢… ë¦¬ë·° ê°œìˆ˜:", len(df_clean))
df_clean.sample(5)

# âœ… [4] ë¦¬ë·° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (spaCy ê¸°ë°˜)
# âœ… [4-1]: ì¶•ì•½í˜• ë³µì› ë° ë¦¬ìŠ¤íŠ¸ ìƒì„± (ê²€ì¦ í¬í•¨)
import pandas as pd
import contractions
import re
from tqdm import tqdm

# âœ… 1ë‹¨ê³„: ê¹¨ì§„ ì¶•ì•½í˜• ë³µì› í•¨ìˆ˜ ì •ì˜
def fix_broken_contractions(text):
    broken_patterns = [
        (r"\b(can)\s+t\b", r"\1't"),
        (r"\b(won)\s+t\b", r"\1't"),
        (r"\b(don)\s+t\b", r"\1't"),
        (r"\b(doesn)\s+t\b", r"\1't"),
        (r"\b(didn)\s+t\b", r"\1't"),
        (r"\b(shouldn)\s+t\b", r"\1't"),
        (r"\b(wasn)\s+t\b", r"\1't"),
        (r"\b(weren)\s+t\b", r"\1't"),
        (r"\b(couldn)\s+t\b", r"\1't"),
        (r"\b(i)\s+m\b", r"\1'm"),
        (r"\b(it)\s+s\b", r"\1's"),
        (r"\b(you)\s+re\b", r"\1're"),
        (r"\b(we)\s+re\b", r"\1're"),
        (r"\b(they)\s+re\b", r"\1're"),
        (r"\b(who)\s+s\b", r"\1's"),
        (r"\b(that)\s+s\b", r"\1's"),
    ]
    for pattern, repl in broken_patterns:
        text = re.sub(pattern, repl, text, flags=re.IGNORECASE)
    return text

# âœ… ì›ë³¸ í…ìŠ¤íŠ¸ ë¡œë”©
texts = df_clean["Review"].fillna("").tolist()

# âœ… 1ë‹¨ê³„ ì ìš©: ê¹¨ì§„ ì¶•ì•½í˜• ë³µì›
fixed_contraction_texts = [fix_broken_contractions(text) for text in tqdm(texts, desc="ğŸ”§ ê¹¨ì§„ ì¶•ì•½í˜• ë³µì›")]

# âœ… 2ë‹¨ê³„ ì ìš©: contractions íŒ¨í‚¤ì§€ ë³µì›
expanded_texts = [contractions.fix(text) for text in tqdm(fixed_contraction_texts, desc="ğŸ”§ í‘œì¤€ ì¶•ì•½í˜• ë³µì›")]

# âœ… ê²°ê³¼ ì €ì¥
df_clean["Expanded_Review"] = expanded_texts
print("âœ… ì¶•ì•½í˜• ë³µì› ì™„ë£Œ!")

# âœ… ê²€ì¦ (ì˜ˆì‹œ í‚¤ì›Œë“œ í™•ì¸)
keywords = [
    ("don t", "don't"), ("wasn t", "wasn't"), ("can t", "can't"),
    ("it s", "it's"), ("i m", "i'm"), ("won t", "won't")
]

for wrong, fixed in keywords:
    wrong_count = sum(wrong in t.lower() for t in texts)
    fixed_count = sum(fixed in t.lower() for t in df_clean["Expanded_Review"])
    print(f"ğŸ” '{wrong}' í¬í•¨ ì›ë¬¸ ìˆ˜: {wrong_count}")
    print(f"âœ… '{fixed}' ë³µì› í›„ í¬í•¨ ìˆ˜: {fixed_count}")
    print("-" * 50)

# âœ… ì˜ˆì‹œ ë¹„êµ ì¶œë ¥
SAMPLE_INDEXES = [9, 14, 24, 25, 50]
for idx in SAMPLE_INDEXES:
    print(f"[Index {idx}]")
    print("ì›ë¬¸  :", texts[idx])
    print("ë³µì›í›„:", df_clean['Expanded_Review'][idx])
    print("------------------------------------------------------------")

# âœ… [4-2]: spaCy ì „ì²˜ë¦¬ + ë¶€ì •ì–´ ë° ê°ì • ë‹¨ì–´ ë³´ì¡´ + ì €ì¥
import spacy
from tqdm import tqdm

# âœ… ì‚¬ì „ ì •ì˜: ë³´ì¡´í•  ë‹¨ì–´ë“¤
NEGATION_WORDS = {"not", "no", "never", "nor"}
SENTIMENT_WORDS = {"good", "bad", "clean", "dirty", "friendly", "unfriendly", "helpful", "rude"}

# âœ… ì²˜ë¦¬ ëŒ€ìƒ
texts = df_clean['Expanded_Review'].fillna("").tolist()

# âœ… ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
processed_texts = []

print("ğŸ”§ spaCy pipe ì „ì²˜ë¦¬ ì‹œì‘...")

# âœ… pipe ë°©ì‹ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
for doc in tqdm(nlp.pipe(texts, batch_size=2000, n_process=1), total=len(texts)):
    try:
        tokens = [
            token.lemma_.lower()
            for token in doc
            if (
                not token.is_stop
                or token.text.lower() in NEGATION_WORDS
                or token.text.lower() in SENTIMENT_WORDS
            )
            and not token.is_punct
            and not token.is_space
        ]
        processed_texts.append(' '.join(tokens))
    except Exception as e:
        processed_texts.append("")
        print("âš ï¸ ì „ì²˜ë¦¬ ì˜¤ë¥˜:", e)

# âœ… ê²°ê³¼ ì €ì¥
df_clean["Processed"] = processed_texts

# âœ… CSVë¡œ ì €ì¥
output_path = "dataset/Hotel_Reviews_CLEAN.csv"
df_clean.to_csv(output_path, index=False)

print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ ë° ì €ì¥ ì™„ë£Œ!")
print(df_clean[["Expanded_Review", "Processed"]].sample(3))

# ğŸ§ª [4-3]: ì „ì²˜ë¦¬ í’ˆì§ˆ ê²€ì¦ ì½”ë“œ
# âœ… ê²€ì¦ ëŒ€ìƒ ë‹¨ì–´ ëª©ë¡
NEGATION_WORDS = {"not", "no", "never", "nor"}
SENTIMENT_WORDS = {"good", "bad", "clean", "dirty", "friendly", "unfriendly", "helpful", "rude"}

# âœ… ë¶€ì •ì–´ í¬í•¨ ë¦¬ë·° ìˆ˜ í™•ì¸
print("ğŸ” ë¶€ì •ì–´ í¬í•¨ ë¦¬ë·° ìˆ˜:")
for word in NEGATION_WORDS:
    count = df_clean["Processed"].str.contains(rf"\b{word}\b", case=False).sum()
    print(f"{word:>6}: {count}")

print("\nğŸ” ê°ì • ë‹¨ì–´ í¬í•¨ ë¦¬ë·° ìˆ˜:")
for word in SENTIMENT_WORDS:
    count = df_clean["Processed"].str.contains(rf"\b{word}\b", case=False).sum()
    print(f"{word:>10}: {count}")

# âœ… ì˜ˆì‹œ ì¶”ì¶œ
print("\nğŸ” ì˜ˆì‹œ ë¦¬ë·° (ë¶€ì •ì–´/ê°ì •ì–´ í¬í•¨):")
mask = df_clean["Processed"].str.contains(r"\b(no|not|never|good|bad|helpful|friendly|dirty)\b", case=False)
print(df_clean[mask][["Expanded_Review", "Processed"]].sample(5, random_state=42))